
some OMP syntax
!$omp parallel do
!$omp end parallel

IBM
serial:    xlf90      -qsmp=omp
mpi:       mpixf90_r  -qsmp=omp:noauto

           _r:   threads safety  (eventually used also for xlf90)
           smp:  shared memory paradigm
           noauto:  no auto shared mem capability to the compiler (otherwise it is a mess)

           OMP_NUM_THREADS=...   (beware the default value!)

           - essl  vs  esslsmp


xlfXX      XX= , 90, 95, 2000, 2003    (fron-end nodes)

bgxlfXX    back-end nodes

mpixlfXX   MPI + xlf + bg (?)


OMP:

C/C++
#pragma

F90
!$omp 


fork
!$omp parallel
join
!$omp end parallel     (impose a barrier)


!$omp parallel
!$omp do
    ! this already does something that is
    ! similar to a forall  (or better, this is assumed to be the case)
!$omp end do
!$omp end parallel


! fork-join costs
!
!$omp parallel
...
!$omp master
    ! emulates a "serial" region, without join-fork again
!$omp end master
...
!$omp end parallel


! fork-join costs
!
!$omp parallel
...
!$omp critical [name]
    ! only one thread at a time enters here
!$omp end critical
!
...
!
!$omp barrier    (syncronize all the threads)
                 this is very useful in OMP
...
!
!$omp atomic     (if x is shared I don't know which thread is writing
                  at the memory location... multiple thr here mess up
                  atomic avoids this)
  x=x+1
  ! note here x is know by all the threads
!
...
!
!$omp do private(tmp)
!
! note that tmp is private and is a local copy in memory
! i is also automatically private
!
do i = 1, n
    !
    tmp=func(b(i))
    !
    a(i) = b(i) + tmp
    ! 
enddo
!
!$omp end do

! reduction
!
!$omp do reduction (+:x)
do i = 1, n
   x = x + a(i)
enddo
!$omp end do

!$omp end parallel


basic vars

export OMP_NUM_THREADS=8
export OMP_SCHEDULE="guided,4"
export OMP_STACKSIZE=

the stack of each thread is replicated

========================================

call OMP_SET_NUM_THREADS(4)

MPI master approach 
   mpi calls are only outside the OMP parallel regions

if one uses a MPI_INIT, this is automatically supposed to be 
MASTER ONLY (abnd the calling mpi in a parallel regio is an error)

MPI-2  has OMP support, one needs to use
   MPI_INIT_THREAD("required","provided",ierr)

=========

NTG:  2-8
NDIAG:  up to the number of procs in the pool
        (1000 nbns, 256 Ndiag)

